{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2845e5cb-deff-472f-afeb-d3c7f6555941",
   "metadata": {},
   "source": [
    "# Vision transformer with quantum self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd10d3-6c47-4893-8242-c7d941e270ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524376dd-3858-4308-b1e1-3a3f0d78a12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 09:15:44.634416: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-18 09:15:44.634436: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f7dfc0-c6ad-461a-b73d-0e4407671669",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882f4c7-d86a-4880-8306-01bd27ca0a54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50552072-eceb-48af-ab55-d11b9ba57e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"small_quark_gluon_candr\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f994767-f3a0-485e-bfc0-6ee60080bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = f.get('X')\n",
    "y_train = f.get('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68eb70f-edae-45da-adbc-76cfa62a0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train[:], y_train[:], test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79505b79-a8a0-4202-b3b1-556ffef8e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_val = x_val.reshape(x_val.shape + (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada53a7d-d911-4092-a11c-48a83aba127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "div1 = np.max(x_train, axis=(1,2)).reshape((x_train.shape[0],1,1,1))\n",
    "div1[div1 == 0] = 1\n",
    "x_train = x_train / div1\n",
    "\n",
    "div1 = np.max(x_val, axis=(1,2)).reshape((x_val.shape[0],1,1,1))\n",
    "div1[div1 == 0] = 1\n",
    "x_val = x_val / div1\n",
    "\n",
    "x_test = x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aaf93e9-3bfb-4abd-bd0c-22796edb09e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (40, 40, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e4445-c9bc-4d2d-8dec-6f41f0be9ac4",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f6455c-4d04-4209-80f6-037c0ac71b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "weight_decay = learning_rate * 10\n",
    "batch_size = 32\n",
    "num_epochs = 25\n",
    "image_size = 40  # We'll resize input images to this size\n",
    "patch_size = 3  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 12\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 3\n",
    "#mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "mlp_head_units = [128, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022051dd-990e-4fdd-a012-8bb2f82a37f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 09:15:47.130393: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-18 09:15:47.130413: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-18 09:15:47.130434: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tomskopfbahnhof): /proc/driver/nvidia/version does not exist\n",
      "2022-09-18 09:15:47.130634: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        #layers.RandomFlip(\"horizontal\"),\n",
    "        #layers.RandomRotation(factor=0.02),\n",
    "        #layers.RandomZoom(\n",
    "        #    height_factor=0.2, width_factor=0.2\n",
    "        #),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2caa60c-e250-4617-807d-535d18875a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "793ec937-9e78-40d4-9adf-7f443f356157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deec315d-5a4c-4116-9e7a-69967bcf9411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 40 X 40\n",
      "Patch size: 3 X 3\n",
      "Patches per image: 169\n",
      "Elements per patch: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC70lEQVR4nO3YwQ1BURBAUV9UoQpNiApUqQLRhCqU4SnAj4WEf8U5y/c2s7mZZKYxxgroWS89ADBPnBAlTogSJ0SJE6I2rz7366NTLnzY5X6a5t5tTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUZulB/gH59v16e2w3X19Dn6LzQlR4oQocUKUOCHKQegLHH94h80JUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidETWOMpWcAZticECVOiBInRIkTosQJUeKEqAeQaA9eJHa4IAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADnCAYAAADy1tHpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADf0lEQVR4nO3cwUlEQRBAQf9iFEZhEmIERmkEYhJGYRh+rwMyCyOO+qDquAtNXx4Ni+NxnucN0HH56wWANaKFGNFCjGghRrQQc3vty4fL07d/Wn79eD5+cs5/mWGXfTPsMt9l5NJCjGghRrQQI1qIES3EiBZiRAsxooUY0UKMaCFGtBAjWogRLcQc/kcUtLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYq7+cQX7vLy/ffns8e7+1/egx6WFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiPEIfnGGXfbNsMt8l5FLCzGihRjRQoxoIUa0ECNaiBEtxIgWYkQLMaKFGNFCjGghRrQQ4xE8xLi0ECNaiBEtxIgWYkQLMaKFmE/iM9LR371a8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 169 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1b65a94-8f0a-40bc-9a7b-e9e3dc57fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a3739e-8f1c-4a23-84d0-9fbaca78fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane.transforms.batch_input import batch_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e135994-188f-4daa-acdc-f6fd72770eb6",
   "metadata": {},
   "source": [
    "Quantum self attention implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6757c424-746d-4907-b399-d87339be75db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QSAL(layers.Layer):\n",
    "    def __init__(self, input_shape, output_dim, batch_size, device, layers, diff_method=\"best\"):\n",
    "        super(QSAL, self).__init__()\n",
    "\n",
    "        self.in_shape = input_shape\n",
    "        \n",
    "        self.Uquery =  qml.QNode(self.U_circ, device, diff_method=diff_method)  \n",
    "        self.Ukey =  qml.QNode(self.U_circ, device, diff_method=diff_method)  \n",
    "        self.Uvalue =  qml.QNode(self.val_circ, device, diff_method=diff_method)  \n",
    "        \n",
    "        #self.Uquery = batch_input(self.Uquery, argnum=0)\n",
    "        #self.Ukey = batch_input(self.Ukey, argnum=0)\n",
    "        #self.Uvalue = batch_input(self.Uvalue, argnum=0)\n",
    "        \n",
    "        self.Uquery.interface = \"tf\"\n",
    "        self.Ukey.interface = \"tf\"\n",
    "        self.Uvalue.interface = \"tf\"\n",
    "        \n",
    "        self.Uquery_weights_size = (layers,input_shape[-1])\n",
    "        self.Ukey_weights_size = (layers,input_shape[-1])\n",
    "        self.Uvalue_weights_size = (layers,input_shape[-1])\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super(QSAL, self).__init__(dynamic=True)\n",
    "    \n",
    "    def U_circ(self, inputs, weights):\n",
    "        self.encoding(inputs)\n",
    "        \n",
    "        qml.BasicEntanglerLayers(weights=weights, wires=range(self.in_shape[-1]))\n",
    "        \n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "    \n",
    "    def val_circ(self, inputs, weights):\n",
    "        \n",
    "        self.encoding(inputs)\n",
    "        \n",
    "        qml.BasicEntanglerLayers(weights=weights, wires=range(self.in_shape[-1]))\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(x)) for x in range(self.in_shape[-1])]\n",
    "        \n",
    "    def encoding(self, inputs):\n",
    "\n",
    "        qml.AngleEmbedding(inputs, wires=range(self.in_shape[-1]))\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Initializes the QNode weights.\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple or tf.TensorShape): shape of input data\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.Uquery_weights = self.add_weight(name=\"query\", shape=self.Uquery_weights_size)\n",
    "        self.Ukey_weights = self.add_weight(name=\"key\", shape=self.Ukey_weights_size)\n",
    "        self.Uvalue_weights = self.add_weight(name=\"value\", shape=self.Uvalue_weights_size)       \n",
    "\n",
    "        super().build(input_shape)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #Zqs = tf.convert_to_tensor([self.Uquery(i, self.Uquery_weights) for i in inputs])\n",
    "        #Zks = tf.convert_to_tensor([self.Ukey(i, self.Ukey_weights) for i in inputs])\n",
    "        #Ps = tf.convert_to_tensor([self.Uvalue(i, self.Uvalue_weights) for i in inputs])\n",
    "        #Zqs = self.Uquery(inputs, self.Uquery_weights)\n",
    "        #Zks = self.Ukey(inputs, self.Ukey_weights)\n",
    "        #Ps = self.Uvalue(inputs, self.Uvalue_weights)\n",
    "        outs = []\n",
    "        for x in tf.unstack(inputs):\n",
    "            Zqs = []\n",
    "            Zks = []\n",
    "            Ps = []\n",
    "            for i in tf.unstack(x):\n",
    "                Zqs.append(self.Uquery(i, self.Uquery_weights))\n",
    "                Zks.append(self.Ukey(i, self.Ukey_weights))\n",
    "                Ps.append(self.Uvalue(i, self.Uvalue_weights))\n",
    "            Zqs = tf.stack(Zqs)\n",
    "            Zks = tf.stack(Zks)\n",
    "            Ps = tf.stack(Ps)\n",
    "        #Zks = tf.map_fn(lambda i: self.Ukey(inputs[:,i,:]), tf.convert_to_tensor(range(inputs.shape[1])), dtype=tf.float32)\n",
    "        #Ps = tf.map_fn(lambda i: self.Uvalue(inputs[:,i,:]), range(inputs.shape[1]), dtype=tf.float32)\n",
    "            Zqs_expand = tf.expand_dims(Zqs, axis=-1)\n",
    "            Zqs_expand = tf.repeat(Zqs_expand, repeats=Zqs_expand.shape[0], axis=1)\n",
    "            Zks_expand = tf.expand_dims(Zks, axis=0)\n",
    "            Zks_expand = tf.repeat(Zks_expand, repeats=Zks_expand.shape[1], axis=0)\n",
    "            alphas = tf.math.exp(-tf.math.square(Zqs_expand - Zks_expand)) \n",
    "            alphas = alphas / tf.reduce_sum(alphas, axis=1)\n",
    "            alpha_prod = tf.linalg.matmul(alphas, Ps)\n",
    "            outs.append(alpha_prod)\n",
    "            #outputs = tf.convert_to_tensor([i + alpha_prod[j] for i,j in enumerate(inputs)])\n",
    "        outputs = tf.stack(outs)\n",
    "        return outputs \n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(self.output_dim)#.concatenate(self.output_dim)\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        detail = \"<Quantum Keras Layer: func={}>\"\n",
    "        return detail.format(self.qnode.func.__name__)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    _input_arg = \"inputs\"\n",
    "\n",
    "    @property\n",
    "    def input_arg(self):\n",
    "        \"\"\"Name of the argument to be used as the input to the Keras\n",
    "        `Layer <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer>`__. Set to\n",
    "        ``\"inputs\"``.\"\"\"\n",
    "        return self._input_arg\n",
    "\n",
    "    @staticmethod\n",
    "    def set_input_argument(input_name = \"inputs\"):\n",
    "        \"\"\"\n",
    "        Set the name of the input argument.\n",
    "\n",
    "        Args:\n",
    "            input_name (str): Name of the input argument\n",
    "        \"\"\"\n",
    "        KerasLayer._input_arg = input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94faee44-1b0d-4b46-bee1-a860e38cc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev1 = qml.device('lightning.qubit', wires=projection_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "750a79a1-117f-47c7-bccf-daf7f44ad48e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier(batch_size):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        #attention_output = layers.MultiHeadAttention(\n",
    "            #num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        #)(x1, x1)\n",
    "        attention_output = QSAL(x1.shape, x1.shape, batch_size, dev1, 2, diff_method=\"adjoint\")(x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.4)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n",
    "    # Classify outputs.\n",
    "    #logits = layers.Dense(1, activation='sigmoid')(features)\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79f555-04cf-4393-b076-8f2f0bb4437c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "10/10 [==============================] - 11691s 1159s/step - loss: 1.5936 - accuracy: 0.4562 - val_loss: 0.8283 - val_accuracy: 0.3839\n",
      "Epoch 2/25\n",
      "10/10 [==============================] - 57745s 5835s/step - loss: 1.4030 - accuracy: 0.4969 - val_loss: 0.8023 - val_accuracy: 0.5268\n",
      "Epoch 3/25\n",
      "10/10 [==============================] - 26672s 2328s/step - loss: 1.3179 - accuracy: 0.4688 - val_loss: 0.8566 - val_accuracy: 0.5089\n",
      "Epoch 4/25\n",
      "10/10 [==============================] - 13020s 1330s/step - loss: 1.1041 - accuracy: 0.5406 - val_loss: 0.9250 - val_accuracy: 0.5000\n",
      "Epoch 5/25\n",
      " 1/10 [==>...........................] - ETA: 3:26:09 - loss: 1.4399 - accuracy: 0.5000"
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay, clipnorm=1.0\n",
    "    )\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        #\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        #loss = keras.losses.BinaryCrossentropy(),\n",
    "        #metrics = [keras.metrics.BinaryAccuracy(), keras.metrics.AUC()]\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            #keras.metrics.AUC()\n",
    "            #keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    #model.build(input_shape=(40,40))\n",
    "    #model.summary()\n",
    "\n",
    "    #checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    #checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    #    checkpoint_filepath,\n",
    "    #    monitor=\"val_accuracy\",\n",
    "    #    save_best_only=True,\n",
    "    #    save_weights_only=True,\n",
    "    #)\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.001)\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.001,\n",
    "        callbacks=[es],\n",
    "        shuffle=True,\n",
    "        steps_per_epoch=10\n",
    "        #callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    #model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy = model.evaluate(x_test[:100], y_val[:100])\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    #print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier(batch_size)\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c605d56-09c3-433e-9c53-a1d50bfef88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_classifier.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfq",
   "language": "python",
   "name": "tfq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
