{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7bd10d3-6c47-4893-8242-c7d941e270ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524376dd-3858-4308-b1e1-3a3f0d78a12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 10:42:18.627051: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-15 10:42:18.627071: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882f4c7-d86a-4880-8306-01bd27ca0a54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50552072-eceb-48af-ab55-d11b9ba57e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"small_quark_gluon_40L\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f994767-f3a0-485e-bfc0-6ee60080bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = f.get('X')\n",
    "y_train = f.get('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c68eb70f-edae-45da-adbc-76cfa62a0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train[:], y_train[:], test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79505b79-a8a0-4202-b3b1-556ffef8e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_val = x_val.reshape(x_val.shape + (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada53a7d-d911-4092-a11c-48a83aba127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "div1 = np.max(x_train, axis=(1,2)).reshape((x_train.shape[0],1,1,1))\n",
    "div1[div1 == 0] = 1\n",
    "x_train = x_train / div1\n",
    "\n",
    "div1 = np.max(x_val, axis=(1,2)).reshape((x_val.shape[0],1,1,1))\n",
    "div1[div1 == 0] = 1\n",
    "x_val = x_val / div1\n",
    "\n",
    "x_test = x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aaf93e9-3bfb-4abd-bd0c-22796edb09e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (40, 40, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e4445-c9bc-4d2d-8dec-6f41f0be9ac4",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f6455c-4d04-4209-80f6-037c0ac71b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.005\n",
    "batch_size = 512\n",
    "num_epochs = 100\n",
    "image_size = 40  # We'll resize input images to this size\n",
    "patch_size = 5  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 49\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022051dd-990e-4fdd-a012-8bb2f82a37f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 10:42:26.212822: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-15 10:42:26.212866: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-15 10:42:26.212887: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tomskopfbahnhof): /proc/driver/nvidia/version does not exist\n",
      "2022-09-15 10:42:26.213153: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-15 10:42:26.220922: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4064736000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        #layers.RandomFlip(\"horizontal\"),\n",
    "        #layers.RandomRotation(factor=0.02),\n",
    "        #layers.RandomZoom(\n",
    "        #    height_factor=0.2, width_factor=0.2\n",
    "        #),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2caa60c-e250-4617-807d-535d18875a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793ec937-9e78-40d4-9adf-7f443f356157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deec315d-5a4c-4116-9e7a-69967bcf9411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 40 X 40\n",
      "Patch size: 5 X 5\n",
      "Patches per image: 64\n",
      "Elements per patch: 25\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC70lEQVR4nO3YSw0CQRBAQZagAhWYIChAJQoIJlCBDAYBbLjw2RdSdZy59OWlk57GGCugZ730AMA8cUKUOCFKnBAlTojavPrcr49OufBll/tpmnu3OSFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSozdID8Fnn2/Xp7bDd/XwO3mdzQpQ4IUqcECVOiHIQ+jOOP//D5oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQNY0xlp4BmGFzQpQ4IUqcECVOiBInRIkToh6noQ9eDC0AxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOwAAADnCAYAAAAdFLrXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADSklEQVR4nO3dwUlDURBA0UmwCquwCbECq7QCsQmrsAxjAUIQ/Pq4eecsdTGZxWWICO90uVwGaDiv/gDAzwkWQgQLIYKFEMFCyN21Xz6en//0T8hvny+n1TPtePy8FTN32HHGhYUUwUKIYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyEnDyGBR0uLIQIFkIECyGChRBv69jx8HkrZu6w44wLCymChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoUQwUKIYCFEsBAiWAjxtg6EuLAQIlgIESyECBZCvK1jx8PnrZi5w44zLiykCBZCBAshgoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhxNs6EOLCQohgIUSwECJYCPG2jh0Pn7di5g47zriwkCJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRBv60CICwshgoUQwUKIpzrsePi8FTN32HHGhYUUwUKIYCFEsBAiWAgRLIQIFkIECyGChZCr/+nEbXj9eP/2s6f7h3//HPyeCwshgoUQwUKI77Ab8H31driwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQ4m0dCHFhIUSwECJYCPG2jh0Pn7di5g47zriwkCJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRBPdUCICwshgoUQwUKIYCHE2zp2PHzeipk77DjjwkKKYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCvK0DIS4shAgWQgQLIYKFEG/r2PHweStm7rDjjAsLKYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCPG2DoS4sBAiWAgRLIQIFkIECyGChZAvAAsgX4qcNXoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 64 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1b65a94-8f0a-40bc-9a7b-e9e3dc57fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "750a79a1-117f-47c7-bccf-daf7f44ad48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f79f555-04cf-4393-b076-8f2f0bb4437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 10:42:42.597986: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3658259200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  21/1117 [..............................] - ETA: 1:18:15 - loss: 1.9146 - accuracy: 0.4944"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            #keras.metrics.AUC()\n",
    "            #keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    #checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    #checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    #    checkpoint_filepath,\n",
    "    #    monitor=\"val_accuracy\",\n",
    "    #    save_best_only=True,\n",
    "    #    save_weights_only=True,\n",
    "    #)\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[es]\n",
    "        #callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    #model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy = model.evaluate(x_test, y_val)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    #print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd423d-592c-4ade-85bd-349c9a07d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3bac72-50c8-4aec-a45a-0ffd4eefc92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3a1012b-81e4-4095-b9b3-148fd48439e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 40, 40, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 40, 40, 1)   3           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " patches_1 (Patches)            (None, None, 25)     0           ['data_augmentation[0][0]']      \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 64, 49)       4410        ['patches_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 64, 49)      98          ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 64, 49)      39053       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 64, 49)       0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 64, 49)      98          ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64, 98)       4900        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64, 98)       0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64, 49)       4851        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64, 49)       0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 64, 49)       0           ['dropout_1[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 64, 49)      98          ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 64, 49)      39053       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 49)       0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 64, 49)      98          ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64, 98)       4900        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64, 98)       0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64, 49)       4851        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64, 49)       0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64, 49)       0           ['dropout_3[0][0]',              \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 64, 49)      98          ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 64, 49)      39053       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 64, 49)       0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 64, 49)      98          ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64, 98)       4900        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64, 98)       0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64, 49)       4851        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64, 49)       0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 64, 49)       0           ['dropout_5[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 64, 49)      98          ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 64, 49)      39053       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 64, 49)       0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 64, 49)      98          ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64, 98)       4900        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64, 98)       0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64, 49)       4851        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64, 49)       0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 64, 49)       0           ['dropout_7[0][0]',              \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 64, 49)      98          ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 64, 49)      39053       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 64, 49)       0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 64, 49)      98          ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 64, 98)       4900        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64, 98)       0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64, 49)       4851        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 64, 49)       0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 64, 49)       0           ['dropout_9[0][0]',              \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 64, 49)      98          ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 64, 49)      39053       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 64, 49)       0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 64, 49)      98          ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64, 98)       4900        ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 64, 98)       0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 64, 49)       4851        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 64, 49)       0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 64, 49)       0           ['dropout_11[0][0]',             \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 64, 49)      98          ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 64, 49)      39053       ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 64, 49)       0           ['multi_head_attention_6[0][0]', \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 64, 49)      98          ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 64, 98)       4900        ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 64, 98)       0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 64, 49)       4851        ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 64, 49)       0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 64, 49)       0           ['dropout_13[0][0]',             \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 64, 49)      98          ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 64, 49)      39053       ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 64, 49)       0           ['multi_head_attention_7[0][0]', \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 64, 49)      98          ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 64, 98)       4900        ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 64, 98)       0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 64, 49)       4851        ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 64, 49)       0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 64, 49)       0           ['dropout_15[0][0]',             \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 64, 49)      98          ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3136)         0           ['layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 3136)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 2048)         6424576     ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 2048)         0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 1024)         2098176     ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 1024)         0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 2)            2050        ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,921,313\n",
      "Trainable params: 8,921,310\n",
      "Non-trainable params: 3\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f09e36-a9bd-4600-a7e5-dc796e605be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8aaad544-6844-4d26-a8af-d3aff3f704b3",
   "metadata": {},
   "source": [
    "392/392 [==============================] - 727s 2s/step - loss: 0.7680 - accuracy: 0.5647 - val_loss: 0.6524 - val_accuracy: 0.6455\n",
    "Epoch 2/100\n",
    "392/392 [==============================] - 710s 2s/step - loss: 0.6433 - accuracy: 0.6374 - val_loss: 0.6345 - val_accuracy: 0.6508\n",
    "Epoch 3/100\n",
    "392/392 [==============================] - 706s 2s/step - loss: 0.6388 - accuracy: 0.6429 - val_loss: 0.6280 - val_accuracy: 0.6495\n",
    "Epoch 4/100\n",
    "392/392 [==============================] - 685s 2s/step - loss: 0.6354 - accuracy: 0.6486 - val_loss: 0.6410 - val_accuracy: 0.6345\n",
    "Epoch 5/100\n",
    "392/392 [==============================] - 686s 2s/step - loss: 0.6331 - accuracy: 0.6488 - val_loss: 0.6284 - val_accuracy: 0.6484\n",
    "Epoch 6/100\n",
    "392/392 [==============================] - 694s 2s/step - loss: 0.6310 - accuracy: 0.6533 - val_loss: 0.6300 - val_accuracy: 0.6401\n",
    "Epoch 7/100\n",
    "392/392 [==============================] - 687s 2s/step - loss: 0.6294 - accuracy: 0.6546 - val_loss: 0.6237 - val_accuracy: 0.6617\n",
    "Epoch 8/100\n",
    "392/392 [==============================] - 685s 2s/step - loss: 0.6280 - accuracy: 0.6553 - val_loss: 0.6549 - val_accuracy: 0.6207\n",
    "Epoch 9/100\n",
    "392/392 [==============================] - 678s 2s/step - loss: 0.6284 - accuracy: 0.6570 - val_loss: 0.6250 - val_accuracy: 0.6534\n",
    "Epoch 10/100\n",
    "392/392 [==============================] - 678s 2s/step - loss: 0.6263 - accuracy: 0.6584 - val_loss: 0.6180 - val_accuracy: 0.6639\n",
    "Epoch 11/100\n",
    "392/392 [==============================] - 680s 2s/step - loss: 0.6249 - accuracy: 0.6598 - val_loss: 0.6209 - val_accuracy: 0.6592\n",
    "Epoch 12/100\n",
    "392/392 [==============================] - 674s 2s/step - loss: 0.6253 - accuracy: 0.6596 - val_loss: 0.6302 - val_accuracy: 0.6526\n",
    "Epoch 13/100\n",
    "392/392 [==============================] - 671s 2s/step - loss: 0.6235 - accuracy: 0.6617 - val_loss: 0.6146 - val_accuracy: 0.6701\n",
    "Epoch 14/100\n",
    "392/392 [==============================] - 676s 2s/step - loss: 0.6242 - accuracy: 0.6590 - val_loss: 0.6153 - val_accuracy: 0.6674\n",
    "Epoch 15/100\n",
    "392/392 [==============================] - 676s 2s/step - loss: 0.6235 - accuracy: 0.6624 - val_loss: 0.6160 - val_accuracy: 0.6638\n",
    "Epoch 16/100\n",
    "392/392 [==============================] - 677s 2s/step - loss: 0.6228 - accuracy: 0.6609 - val_loss: 0.6184 - val_accuracy: 0.6613\n",
    "Epoch 17/100\n",
    "392/392 [==============================] - 674s 2s/step - loss: 0.6225 - accuracy: 0.6628 - val_loss: 0.6153 - val_accuracy: 0.6664\n",
    "Epoch 18/100\n",
    "392/392 [==============================] - 678s 2s/step - loss: 0.6216 - accuracy: 0.6625 - val_loss: 0.6177 - val_accuracy: 0.6615"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfq",
   "language": "python",
   "name": "tfq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
